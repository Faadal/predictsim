\documentclass{article}
% The file ijcai11.sty is the style file for IJCAI-11 (same as ijcai07.sty).
\usepackage{ijcai11}

% Use the postscript times font!
\usepackage{times}
\usepackage[utf8x]{inputenc}
\usepackage{datetime}
\usepackage{graphicx}

\usepackage{mathptmx}
\usepackage[T1]{fontenc}

% the following package is optional:
\usepackage{latexsym}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


\title{Master 1 MoSIG Research Project Report\\
Learning Job Runtimes in Homogenous HPC Systems}
\author{Valentin Reis\\
Supervised by: Denis Trystram \& Eric Gaussier.\\
}

\begin{document}

\maketitle

{% Mosig student
  {\hbox to0pt{\vbox{\baselineskip=10dd\hrule\hbox
to\hsize{\vrule\kern3pt\vbox{\kern3pt
\hbox{{\small I understand what plagiarism entails and I declare that this report }}
\hbox{{\small is my own, original work. }}
\hbox{{\small Name, date and signature: }}
\kern3pt
}\hfil%\kern3pt
\vrule
}\hrule}
}}
}


\begin{abstract}
  Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod
  tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At
  vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren,
  no sea takimata sanctus est Lorem ipsum dolor sit amet.
\end{abstract}

\section{Introduction}
High Performance Computing (HPC) systems are complex machinery at the frontier between (research in-a retirer?) scheduling and systems engineering. We outline the two main difficulties that resource management software in this field have to face.

First, the ephemeral nature, and broad range of existing architectures of such systems make the development and application of theoretical results difficult.
New schemes for distributing resources (e.g.\ memory caches, hard drives, processing units, RAM memory) are ubiquitous and their rate of occurence is high. Moreover, the topology of a HPC system can change on a monthly or weekly basis, with the addition of new hardware.
Finding scheduling and resource management strategies which can adapt to those changes is a current research problem.

In addition, the input data these systems have to work with presents many peculiarities. The nature of the information which users of the system provide is very often loose ( e.g.\ with only upper and/or lower bounds provided ). For instance, and this will be the focus of this paper, the run time of a given job on a specific system is seldom known in advance, but an upper bound may be given by the user.

As a consequence of these difficulties most free, open-source and commercial resource management software use simple heuristics, which can provide bounds on their performance and/or guarantee a few functional properties.
An example of such a heuristic is the First Come First Serve (FCFS) policy to schedule parallel jobs on a homogenous cluster of machines. Among other properties (such as robustness to lack of information about the amount of time jobs will run on the system), this strategy guarantees the avoidance of starvation.

%One of the aspects that must be dealt with in order to apply more sophisticated techniques than the currently existing heuristics to schedule jobs on these systems is the uncertainty in the data provided by the users of those systems.

\subsection{Research Direction}
The general direction we are headed in with this research project is to deal with the input data of the resource management systems.
Accomodating for this data seems separable enough from the actual scheduling problems for work towards this objective to be rewarding.
No innovative ways to query the data from the users will be studied : we will mainly rely on existing logs from HPC systems. Instead, we seek to apply Machine Learning techniques in order to reduce uncertainty of, and extract information and/or structure from, the input data of the HPC systems.
We will be working with the problem of presenting input data in the most valuable way possible to a scheduling algorithm. How to use this data to the fullest will not be discussed.
When assessing the relevance of the specific the information we choose to extract, references from the HPC scheduling litterature will provide ground to stand on.

\subsection{Job runtimes}
Most HPC resource management software (including the SLURM, OpenPBS, OpenLava and OAR software) do ask information about jobs to users, such as topological requests in terms of processing units and memory, the name of the executable, miscellaneous functional requirements and, last but not least, the expected run time of the job. This user-provided estimate of the run time of a job on a specific system will be referred as \textbf{reqtime} in the rest of the paper. Most of these software use the \textbf{reqtime} of a job as an upper bound on its run time, and kill it should \textbf{reqtime} be violated. As a consequence, users overestimate this value, should they choose or be forced to provide it. The following section will present a statistical analysis pertaining to this relationship.

The true run time of a job with respect to a given affected topology is of great interest, as the scheduling policies are highly dependent on this information to provide good solutions \ref{truc}. We will refer to this quantity as the \textbf{runtime} of a job.
It must be clear that in the context of ubiquitous topological heterogeneity, the \textbf{runtime} of a job is only defined with respect to a specific processing environment to which it might be affected.
This can include, and is not limited to, the network topology of the processing units, the availability of shared memory, message passing costs, and the operating system supporting the computations.

\subsection{Problem Statement}
We restrict the broad question of refining the data to a single relevant variable: the \textbf{runtime}. The problem statement we are dealing with is the following.

Given a specific homogenous HPC Cluster with negligible communication costs, how to best predict the value of the \textbf{runtime} of a job?

The choice of dealing with homogenous distributed machines without communication costs in a first approach has the interesting property of separing the data treatment from the scheduling and interaction with the system.
In this case, the \textbf{runtime} becomes an intrinsic attribute of a job. On the contrary to the input data which is always subject to peculiarities of the various systems and software, the \textbf{runtime} is always present as a simple field in workload logs of HPC systems, such as those available at the workload archive [ref to feitelson].
Since we will be inferring runtime from the input data, this will be a supervised learning problem, specifically regression.
We will be careful to only learn our models on logs of homogenous systems, or subset of heterogenous systems which are sizeable enough to learn models from and are homogenous in nature.
As for downplaying the impact of communication costs, we will further restrict our work to machines which do not possess overly complex topology or distribute computing nodes across more than a handful of routers.
In essence, we are targeting large beowulf clusters such in-building desktop computer farms, supercomputers and GPU farms.

\section{Motivation}

\subsection{Importance of \textbf{runtime}}
\label{sub:importance_of_runtime}
Once again, we emphasise the role of the \textbf{runtime} in scheduling tasks. Classical results from scheduling theory use the "clairvoyant" model[ref], where all \textbf{runtimes} of jobs with respect to any affectation on the system being modeled are known.
Informally, if we want to make use of this extensive theoretical body, we must at least reduce uncertainty in this information.
More recent models [ref Trystram FCFS] do [...].
Another approach [ref prob.backfilling] uses predicted probability distributions on the runtimes of jobs in order to take scheduling decisions.
In all cases, reducing uncertainty in \textbf{runtime} is critical to the success of the approach used to schedule jobs.

As mentioned before, existing solutions use much simpler heuristics. We will now focus on a particular system, and outline the reasons why, in the absence of a predictive technique applied the input data, the FCFS heuristic is applied.
% subsection importance_of_runtime (end)

\subsection{\textbf{reqtime} vs \textbf{runtime} on a real system}
\label{sub:reqtime_vs_runtime_on_a_real_system}
The following study is conducted on a log containing 20 months worth of data from the Curie supercomputer operated by the French government-funded research organization CEA (Commissariat à l'Énergie Atomique).
The system has several highly homogenous partitions, to which jobs are allocated according to user preference.
[ref figure] shows the marginal distributions of \textbf{reqtimes} and \textbf{runtimes} on this system.
\begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{../../wallimage-0.png}
        \caption{Marginal Histograms of \textbf{reqtime} and \textbf{runtime} of jobs on the CEA Curie log.}
        \label{fig:_wall_run_for_report_pdf}
\end{figure}





TODO: show the curie log.

% subsection reqtime_vs_runtime_on_a_real_system (end)


\section{State of the art in \textbf{runtime} prediction}


\subsection{Nature of the prediction}
\label{sub:nature_of_the_prediction}

TODO: -what to predict: value?, confidence interval?, distribution? whith which algorithms can we use those?

% subsection nature_of_the_prediction (end)

\subsection{Predicting a value}
\label{sub:predicting_a_value}

TODO: -give the references and explain the historical methods, gibbons historical scheduler, the tsafir et al paper with mean of two last runtimes values userwise..

% subsection predicting_a_value (end)

\subsection{Predicting a distribution}
\label{sub:predicting_a_distribution}

TODO: -give the references and explain the probabilistic backfilling thing..

% subsection predicting_a_distribution (end)


\section{Our approach}
\label{sec:our_approach}

\subsection{Random Forests}
\label{sub:random_forests}
TODO: -explain our approach, why it could lead to better results (external info+signal locality(ref hmm thesis for locality..))
% subsection random_forests (end)

\subsection{Explainability}
\label{sub:explainability}
TODO: -explain one advantage of random forests: discussion about the trees after learning..
% subsection explainability (end)

% section our_approach (end)

\section{Preliminary Results}
\label{sec:preliminary_results}

TODO:results..

% section preliminary_results (end)

\section{Conclusions}
\label{sec:conclusions}

TODO:conclure..

% section conclusions (end)


\section{Acknowledgements}
\label{sec:conclusions}

TODO:remercier..

% section conclusions (end)


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{report}

\end{document}
